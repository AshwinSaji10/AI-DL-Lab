{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ashwin Saji(240984006)"
      ],
      "metadata": {
        "id": "3suulA_EkFvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) MAB Agent Formulation"
      ],
      "metadata": {
        "id": "fKA8_I0WhqXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Multi-Armed Bandit (MAB) problem is a classical decision-making problem where a set of actions (in this case, advertisements) is available, each providing a certain reward, but the actual reward distribution of each action is initially unknown. The goal is to choose actions in a way that maximizes cumulative rewards over time. At each time step, the agent must decide which action to take, balancing the trade-off between exploring new actions to learn their reward distributions (exploration) and exploiting the current best-known action to maximize immediate rewards (exploitation).\n",
        "\n",
        "In the context of the advertising dataset:\n",
        "- There are 10 ads (arms) to choose from.\n",
        "\n",
        "- Each time step represents a user interacting with one ad, either clicking (reward = 1) or not clicking (reward = 0).\n",
        "\n",
        "\n",
        "- The agent must decide which ad to present, aiming to maximize the total clicks (reward) over a set period."
      ],
      "metadata": {
        "id": "1MmHwoN-hyQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "true_ctr = np.random.rand(10)  # 10 ads with random click probabilities\n",
        "\n",
        "def simulate_click(ad_index):\n",
        "    return 1 if np.random.rand() < true_ctr[ad_index] else 0"
      ],
      "metadata": {
        "id": "nFgPl_vli-C0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(true_ctr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh6Evss5jZ5A",
        "outputId": "7d2fb51d-7709-42ec-ecb5-70447f3ee151"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.37454012 0.95071431 0.73199394 0.59865848 0.15601864 0.15599452\n",
            " 0.05808361 0.86617615 0.60111501 0.70807258]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Compute the total rewards after 2000-time steps using the Îµ-greedy action. a. for Îµ=0.01, Îµ=\n",
        "0.3"
      ],
      "metadata": {
        "id": "26a6r2-1iW2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Îµ-Greedy Algorithm\n",
        "def epsilon_greedy(epsilon, time_steps):\n",
        "    n_ads = len(true_ctr)\n",
        "    ad_counts = np.zeros(n_ads)\n",
        "    ad_rewards = np.zeros(n_ads)\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(time_steps):\n",
        "        if np.random.rand() < epsilon:\n",
        "            ad_selected = np.random.choice(n_ads)\n",
        "        else:\n",
        "            ad_selected = np.argmax(ad_rewards / (ad_counts + 1e-5))\n",
        "\n",
        "        reward = simulate_click(ad_selected)\n",
        "        ad_counts[ad_selected] += 1\n",
        "        ad_rewards[ad_selected] += reward\n",
        "        total_reward += reward\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "time_steps_egreedy = 2000\n",
        "\n",
        "total_reward_eps_0_01 = epsilon_greedy(0.01, time_steps_egreedy)\n",
        "total_reward_eps_0_3 = epsilon_greedy(0.3, time_steps_egreedy)\n",
        "\n",
        "print(f\"Îµ-Greedy (Îµ=0.01) Total Reward: {total_reward_eps_0_01}\")\n",
        "print(f\"Îµ-Greedy (Îµ=0.3) Total Reward: {total_reward_eps_0_3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3k0MxQH5iKvu",
        "outputId": "ec1f6f39-56ba-48d7-b242-6af417606c70"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Îµ-Greedy (Îµ=0.01) Total Reward: 1507\n",
            "Îµ-Greedy (Îµ=0.3) Total Reward: 1608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C. Compute the total rewards after 1000-time steps using the Upper-Confidence-Bound action\n",
        "method for c= 1.5, 2"
      ],
      "metadata": {
        "id": "ZNCLzsh2jiNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UCB Algorithm\n",
        "def upper_confidence_bound(c, time_steps):\n",
        "    n_ads = len(true_ctr)\n",
        "    ad_counts = np.zeros(n_ads)\n",
        "    ad_rewards = np.zeros(n_ads)\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(time_steps):\n",
        "        if t < n_ads:\n",
        "            ad_selected = t\n",
        "        else:\n",
        "            ucb_values = (ad_rewards / (ad_counts + 1e-5)) + c * np.sqrt(np.log(t + 1) / (ad_counts + 1e-5))\n",
        "            ad_selected = np.argmax(ucb_values)\n",
        "\n",
        "        reward = simulate_click(ad_selected)\n",
        "        ad_counts[ad_selected] += 1\n",
        "        ad_rewards[ad_selected] += reward\n",
        "        total_reward += reward\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "time_steps_ucb = 1000\n",
        "total_reward_ucb_1_5 = upper_confidence_bound(1.5, time_steps_ucb)\n",
        "total_reward_ucb_2 = upper_confidence_bound(2, time_steps_ucb)\n",
        "print(f\"UCB (c=1.5) Total Reward: {total_reward_ucb_1_5}\")\n",
        "print(f\"UCB (c=2.0) Total Reward: {total_reward_ucb_2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai3S-Cwei2Vc",
        "outputId": "24f05f11-5ee5-4915-9244-9673c891d1c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UCB (c=1.5) Total Reward: 824\n",
            "UCB (c=2.0) Total Reward: 758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "D. For all approaches, explain how the action value estimated compares to the optimal action."
      ],
      "metadata": {
        "id": "vo3nKIrbjvym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Îµ-greedy:\n",
        "\n",
        "- The estimated value of each ad in Îµ-greedy is based on the average reward received from that ad over the course of interactions. In the early stages, Îµ-greedy will explore more randomly, but as time progresses, it will converge towards exploiting the ad with the highest reward. However, the exploration rate (dependent on Îµ) means that even the best ad may not be exploited all the time.\n",
        "- The optimal action is the ad with the highest true average reward. Over time, the agent's estimated action value should converge to the optimal value, but it might not do so efficiently due to the exploration aspect.\n",
        "\n",
        "\n",
        "Upper-Confidence-Bound (UCB):\n",
        "\n",
        "- UCB balances exploration and exploitation more systematically by considering the uncertainty in the estimates. Ads that have been selected less often will have higher uncertainty and may be chosen more frequently. This can lead to better exploration in the early stages but can still converge quickly to the optimal action as the uncertainty is reduced over time.\n",
        "- The estimated value for UCB tends to converge more efficiently toward the optimal action value compared to Îµ-greedy, especially when the\n",
        "ð‘\n",
        "c parameter is set to a higher value, ensuring that exploration is well-managed.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gctvwo4EjwJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YrW0eHTGkB6D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}